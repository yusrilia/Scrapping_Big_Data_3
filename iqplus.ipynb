{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping IQPlus\n",
    "\n",
    "Notebook ini men-scrapping link artikel berita yang ada di http://www.iqplus.info/ menggunakan selenium dan menyimpannya dalam file .txt, kemudian menggunakan beautifulSoup untuk mengambil konten teks artikel dari daftar link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi untuk scrapping link di IQPlus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks(page, category):\n",
    "\n",
    "    # Setup Selenium WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Open the page\n",
    "    url = \"http://www.iqplus.info/news/\"+category+\"/go-to-page,\"+str(page)+\".html\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the \"STOCK NEWS MORE\" section to load\n",
    "    try:\n",
    "        stock_news_section = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h2[contains(text(),'NEWS MORE')]/following-sibling::ul\"))\n",
    "        )\n",
    "\n",
    "        # Extract article links\n",
    "        article_links = set()\n",
    "        articles = stock_news_section.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "        for article in articles:\n",
    "            link = article.get_attribute(\"href\")\n",
    "            if link:\n",
    "                article_links.add(link)\n",
    "\n",
    "        # Append links to a text file\n",
    "        with open((category+\"_links.txt\"), \"a\", encoding=\"utf-8\") as file:\n",
    "            for link in article_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "        print(\"links from page\", page, \"have been added to \"+category+\"_links.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping link Stock News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links from page 141 have been added to stock_news_links.txt\n",
      "links from page 140 have been added to stock_news_links.txt\n",
      "links from page 139 have been added to stock_news_links.txt\n",
      "links from page 138 have been added to stock_news_links.txt\n",
      "links from page 137 have been added to stock_news_links.txt\n",
      "links from page 136 have been added to stock_news_links.txt\n",
      "links from page 135 have been added to stock_news_links.txt\n",
      "links from page 134 have been added to stock_news_links.txt\n",
      "links from page 133 have been added to stock_news_links.txt\n",
      "links from page 132 have been added to stock_news_links.txt\n",
      "links from page 131 have been added to stock_news_links.txt\n",
      "links from page 130 have been added to stock_news_links.txt\n",
      "links from page 129 have been added to stock_news_links.txt\n",
      "links from page 128 have been added to stock_news_links.txt\n",
      "links from page 127 have been added to stock_news_links.txt\n",
      "links from page 126 have been added to stock_news_links.txt\n",
      "links from page 125 have been added to stock_news_links.txt\n",
      "links from page 124 have been added to stock_news_links.txt\n",
      "links from page 123 have been added to stock_news_links.txt\n",
      "links from page 122 have been added to stock_news_links.txt\n",
      "links from page 121 have been added to stock_news_links.txt\n",
      "links from page 120 have been added to stock_news_links.txt\n",
      "links from page 119 have been added to stock_news_links.txt\n",
      "links from page 118 have been added to stock_news_links.txt\n",
      "links from page 117 have been added to stock_news_links.txt\n",
      "links from page 116 have been added to stock_news_links.txt\n",
      "links from page 115 have been added to stock_news_links.txt\n",
      "links from page 114 have been added to stock_news_links.txt\n",
      "links from page 113 have been added to stock_news_links.txt\n",
      "links from page 112 have been added to stock_news_links.txt\n",
      "links from page 111 have been added to stock_news_links.txt\n",
      "links from page 110 have been added to stock_news_links.txt\n",
      "links from page 109 have been added to stock_news_links.txt\n",
      "links from page 108 have been added to stock_news_links.txt\n",
      "links from page 107 have been added to stock_news_links.txt\n",
      "links from page 106 have been added to stock_news_links.txt\n",
      "links from page 105 have been added to stock_news_links.txt\n",
      "links from page 104 have been added to stock_news_links.txt\n",
      "links from page 103 have been added to stock_news_links.txt\n",
      "links from page 102 have been added to stock_news_links.txt\n",
      "links from page 101 have been added to stock_news_links.txt\n",
      "links from page 100 have been added to stock_news_links.txt\n",
      "links from page 99 have been added to stock_news_links.txt\n",
      "links from page 98 have been added to stock_news_links.txt\n",
      "links from page 97 have been added to stock_news_links.txt\n",
      "links from page 96 have been added to stock_news_links.txt\n",
      "links from page 95 have been added to stock_news_links.txt\n",
      "links from page 94 have been added to stock_news_links.txt\n",
      "links from page 93 have been added to stock_news_links.txt\n",
      "links from page 92 have been added to stock_news_links.txt\n",
      "links from page 91 have been added to stock_news_links.txt\n",
      "links from page 90 have been added to stock_news_links.txt\n",
      "links from page 89 have been added to stock_news_links.txt\n",
      "links from page 88 have been added to stock_news_links.txt\n",
      "links from page 87 have been added to stock_news_links.txt\n",
      "links from page 86 have been added to stock_news_links.txt\n",
      "links from page 85 have been added to stock_news_links.txt\n",
      "links from page 84 have been added to stock_news_links.txt\n",
      "links from page 83 have been added to stock_news_links.txt\n",
      "links from page 82 have been added to stock_news_links.txt\n",
      "links from page 81 have been added to stock_news_links.txt\n",
      "links from page 80 have been added to stock_news_links.txt\n",
      "links from page 79 have been added to stock_news_links.txt\n",
      "links from page 78 have been added to stock_news_links.txt\n",
      "links from page 77 have been added to stock_news_links.txt\n",
      "links from page 76 have been added to stock_news_links.txt\n",
      "links from page 75 have been added to stock_news_links.txt\n",
      "links from page 74 have been added to stock_news_links.txt\n",
      "links from page 73 have been added to stock_news_links.txt\n",
      "links from page 72 have been added to stock_news_links.txt\n",
      "links from page 71 have been added to stock_news_links.txt\n",
      "links from page 70 have been added to stock_news_links.txt\n",
      "links from page 69 have been added to stock_news_links.txt\n",
      "links from page 68 have been added to stock_news_links.txt\n",
      "links from page 67 have been added to stock_news_links.txt\n",
      "links from page 66 have been added to stock_news_links.txt\n",
      "links from page 65 have been added to stock_news_links.txt\n",
      "links from page 64 have been added to stock_news_links.txt\n",
      "links from page 63 have been added to stock_news_links.txt\n",
      "links from page 62 have been added to stock_news_links.txt\n",
      "links from page 61 have been added to stock_news_links.txt\n",
      "links from page 60 have been added to stock_news_links.txt\n",
      "links from page 59 have been added to stock_news_links.txt\n",
      "links from page 58 have been added to stock_news_links.txt\n",
      "links from page 57 have been added to stock_news_links.txt\n",
      "links from page 56 have been added to stock_news_links.txt\n",
      "links from page 55 have been added to stock_news_links.txt\n",
      "links from page 54 have been added to stock_news_links.txt\n",
      "links from page 53 have been added to stock_news_links.txt\n",
      "links from page 52 have been added to stock_news_links.txt\n",
      "links from page 51 have been added to stock_news_links.txt\n",
      "links from page 50 have been added to stock_news_links.txt\n",
      "links from page 49 have been added to stock_news_links.txt\n",
      "links from page 48 have been added to stock_news_links.txt\n",
      "links from page 47 have been added to stock_news_links.txt\n",
      "links from page 46 have been added to stock_news_links.txt\n",
      "links from page 45 have been added to stock_news_links.txt\n",
      "links from page 44 have been added to stock_news_links.txt\n",
      "links from page 43 have been added to stock_news_links.txt\n",
      "links from page 42 have been added to stock_news_links.txt\n",
      "links from page 41 have been added to stock_news_links.txt\n",
      "links from page 40 have been added to stock_news_links.txt\n",
      "links from page 39 have been added to stock_news_links.txt\n",
      "links from page 38 have been added to stock_news_links.txt\n",
      "links from page 37 have been added to stock_news_links.txt\n",
      "links from page 36 have been added to stock_news_links.txt\n",
      "links from page 35 have been added to stock_news_links.txt\n",
      "links from page 34 have been added to stock_news_links.txt\n",
      "links from page 33 have been added to stock_news_links.txt\n",
      "links from page 32 have been added to stock_news_links.txt\n",
      "links from page 31 have been added to stock_news_links.txt\n",
      "links from page 30 have been added to stock_news_links.txt\n",
      "links from page 29 have been added to stock_news_links.txt\n",
      "links from page 28 have been added to stock_news_links.txt\n",
      "links from page 27 have been added to stock_news_links.txt\n",
      "links from page 26 have been added to stock_news_links.txt\n",
      "links from page 25 have been added to stock_news_links.txt\n",
      "links from page 24 have been added to stock_news_links.txt\n",
      "links from page 23 have been added to stock_news_links.txt\n",
      "links from page 22 have been added to stock_news_links.txt\n",
      "links from page 21 have been added to stock_news_links.txt\n",
      "links from page 20 have been added to stock_news_links.txt\n",
      "links from page 19 have been added to stock_news_links.txt\n",
      "links from page 18 have been added to stock_news_links.txt\n",
      "links from page 17 have been added to stock_news_links.txt\n",
      "links from page 16 have been added to stock_news_links.txt\n",
      "links from page 15 have been added to stock_news_links.txt\n",
      "links from page 14 have been added to stock_news_links.txt\n",
      "links from page 13 have been added to stock_news_links.txt\n",
      "links from page 12 have been added to stock_news_links.txt\n",
      "links from page 11 have been added to stock_news_links.txt\n",
      "links from page 10 have been added to stock_news_links.txt\n",
      "links from page 9 have been added to stock_news_links.txt\n",
      "links from page 8 have been added to stock_news_links.txt\n",
      "links from page 7 have been added to stock_news_links.txt\n",
      "links from page 6 have been added to stock_news_links.txt\n",
      "links from page 5 have been added to stock_news_links.txt\n",
      "links from page 4 have been added to stock_news_links.txt\n",
      "links from page 3 have been added to stock_news_links.txt\n",
      "links from page 2 have been added to stock_news_links.txt\n",
      "links from page 1 have been added to stock_news_links.txt\n",
      "links from page 0 have been added to stock_news_links.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(141, -1, -1):\n",
    "    getlinks(i, \"stock_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping link Market News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links from page 163 have been added to market_news_links.txt\n",
      "links from page 162 have been added to market_news_links.txt\n",
      "links from page 161 have been added to market_news_links.txt\n",
      "links from page 160 have been added to market_news_links.txt\n",
      "links from page 159 have been added to market_news_links.txt\n",
      "Error: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00B94CA3+225091]\n",
      "\t(No symbol) [0x00AC4DF1]\n",
      "\t(No symbol) [0x00969A7A]\n",
      "\t(No symbol) [0x009A175B]\n",
      "\t(No symbol) [0x009A188B]\n",
      "\t(No symbol) [0x009D7882]\n",
      "\t(No symbol) [0x009BF5A4]\n",
      "\t(No symbol) [0x009D5CB0]\n",
      "\t(No symbol) [0x009BF2F6]\n",
      "\t(No symbol) [0x009979B9]\n",
      "\t(No symbol) [0x0099879D]\n",
      "\tsqlite3_dbdata_init [0x01009A43+4064547]\n",
      "\tsqlite3_dbdata_init [0x0101104A+4094762]\n",
      "\tsqlite3_dbdata_init [0x0100B948+4072488]\n",
      "\tsqlite3_dbdata_init [0x00D0C9A9+930953]\n",
      "\t(No symbol) [0x00AD07C4]\n",
      "\t(No symbol) [0x00ACACE8]\n",
      "\t(No symbol) [0x00ACAE11]\n",
      "\t(No symbol) [0x00ABCA80]\n",
      "\tBaseThreadInitThunk [0x752A5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7756CE3B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7756CDC1+561]\n",
      "\n",
      "links from page 157 have been added to market_news_links.txt\n",
      "links from page 156 have been added to market_news_links.txt\n",
      "links from page 155 have been added to market_news_links.txt\n",
      "links from page 154 have been added to market_news_links.txt\n",
      "links from page 153 have been added to market_news_links.txt\n",
      "links from page 152 have been added to market_news_links.txt\n",
      "links from page 151 have been added to market_news_links.txt\n",
      "links from page 150 have been added to market_news_links.txt\n",
      "links from page 149 have been added to market_news_links.txt\n",
      "links from page 148 have been added to market_news_links.txt\n",
      "links from page 147 have been added to market_news_links.txt\n",
      "links from page 146 have been added to market_news_links.txt\n",
      "links from page 145 have been added to market_news_links.txt\n",
      "links from page 144 have been added to market_news_links.txt\n",
      "links from page 143 have been added to market_news_links.txt\n",
      "links from page 142 have been added to market_news_links.txt\n",
      "links from page 141 have been added to market_news_links.txt\n",
      "links from page 140 have been added to market_news_links.txt\n",
      "links from page 139 have been added to market_news_links.txt\n",
      "links from page 138 have been added to market_news_links.txt\n",
      "links from page 137 have been added to market_news_links.txt\n",
      "links from page 136 have been added to market_news_links.txt\n",
      "links from page 135 have been added to market_news_links.txt\n",
      "links from page 134 have been added to market_news_links.txt\n",
      "links from page 133 have been added to market_news_links.txt\n",
      "links from page 132 have been added to market_news_links.txt\n",
      "links from page 131 have been added to market_news_links.txt\n",
      "links from page 130 have been added to market_news_links.txt\n",
      "links from page 129 have been added to market_news_links.txt\n",
      "links from page 128 have been added to market_news_links.txt\n",
      "links from page 127 have been added to market_news_links.txt\n",
      "links from page 126 have been added to market_news_links.txt\n",
      "links from page 125 have been added to market_news_links.txt\n",
      "links from page 124 have been added to market_news_links.txt\n",
      "links from page 123 have been added to market_news_links.txt\n",
      "links from page 122 have been added to market_news_links.txt\n",
      "links from page 121 have been added to market_news_links.txt\n",
      "links from page 120 have been added to market_news_links.txt\n",
      "links from page 119 have been added to market_news_links.txt\n",
      "links from page 118 have been added to market_news_links.txt\n",
      "links from page 117 have been added to market_news_links.txt\n",
      "links from page 116 have been added to market_news_links.txt\n",
      "links from page 115 have been added to market_news_links.txt\n",
      "links from page 114 have been added to market_news_links.txt\n",
      "links from page 113 have been added to market_news_links.txt\n",
      "links from page 112 have been added to market_news_links.txt\n",
      "links from page 111 have been added to market_news_links.txt\n",
      "links from page 110 have been added to market_news_links.txt\n",
      "links from page 109 have been added to market_news_links.txt\n",
      "links from page 108 have been added to market_news_links.txt\n",
      "links from page 107 have been added to market_news_links.txt\n",
      "links from page 106 have been added to market_news_links.txt\n",
      "links from page 105 have been added to market_news_links.txt\n",
      "links from page 104 have been added to market_news_links.txt\n",
      "links from page 103 have been added to market_news_links.txt\n",
      "links from page 102 have been added to market_news_links.txt\n",
      "links from page 101 have been added to market_news_links.txt\n",
      "links from page 100 have been added to market_news_links.txt\n",
      "links from page 99 have been added to market_news_links.txt\n",
      "links from page 98 have been added to market_news_links.txt\n",
      "links from page 97 have been added to market_news_links.txt\n",
      "links from page 96 have been added to market_news_links.txt\n",
      "links from page 95 have been added to market_news_links.txt\n",
      "links from page 94 have been added to market_news_links.txt\n",
      "links from page 93 have been added to market_news_links.txt\n",
      "links from page 92 have been added to market_news_links.txt\n",
      "links from page 91 have been added to market_news_links.txt\n",
      "links from page 90 have been added to market_news_links.txt\n",
      "links from page 89 have been added to market_news_links.txt\n",
      "links from page 88 have been added to market_news_links.txt\n",
      "links from page 87 have been added to market_news_links.txt\n",
      "links from page 86 have been added to market_news_links.txt\n",
      "links from page 85 have been added to market_news_links.txt\n",
      "links from page 84 have been added to market_news_links.txt\n",
      "links from page 83 have been added to market_news_links.txt\n",
      "links from page 82 have been added to market_news_links.txt\n",
      "links from page 81 have been added to market_news_links.txt\n",
      "links from page 80 have been added to market_news_links.txt\n",
      "links from page 79 have been added to market_news_links.txt\n",
      "links from page 78 have been added to market_news_links.txt\n",
      "links from page 77 have been added to market_news_links.txt\n",
      "links from page 76 have been added to market_news_links.txt\n",
      "links from page 75 have been added to market_news_links.txt\n",
      "links from page 74 have been added to market_news_links.txt\n",
      "links from page 73 have been added to market_news_links.txt\n",
      "links from page 72 have been added to market_news_links.txt\n",
      "links from page 71 have been added to market_news_links.txt\n",
      "links from page 70 have been added to market_news_links.txt\n",
      "links from page 69 have been added to market_news_links.txt\n",
      "links from page 68 have been added to market_news_links.txt\n",
      "links from page 67 have been added to market_news_links.txt\n",
      "links from page 66 have been added to market_news_links.txt\n",
      "links from page 65 have been added to market_news_links.txt\n",
      "links from page 64 have been added to market_news_links.txt\n",
      "links from page 63 have been added to market_news_links.txt\n",
      "links from page 62 have been added to market_news_links.txt\n",
      "links from page 61 have been added to market_news_links.txt\n",
      "links from page 60 have been added to market_news_links.txt\n",
      "links from page 59 have been added to market_news_links.txt\n",
      "links from page 58 have been added to market_news_links.txt\n",
      "links from page 57 have been added to market_news_links.txt\n",
      "links from page 56 have been added to market_news_links.txt\n",
      "links from page 55 have been added to market_news_links.txt\n",
      "links from page 54 have been added to market_news_links.txt\n",
      "links from page 53 have been added to market_news_links.txt\n",
      "links from page 52 have been added to market_news_links.txt\n",
      "links from page 51 have been added to market_news_links.txt\n",
      "links from page 50 have been added to market_news_links.txt\n",
      "links from page 49 have been added to market_news_links.txt\n",
      "links from page 48 have been added to market_news_links.txt\n",
      "links from page 47 have been added to market_news_links.txt\n",
      "links from page 46 have been added to market_news_links.txt\n",
      "links from page 45 have been added to market_news_links.txt\n",
      "links from page 44 have been added to market_news_links.txt\n",
      "links from page 43 have been added to market_news_links.txt\n",
      "links from page 42 have been added to market_news_links.txt\n",
      "links from page 41 have been added to market_news_links.txt\n",
      "links from page 40 have been added to market_news_links.txt\n",
      "links from page 39 have been added to market_news_links.txt\n",
      "links from page 38 have been added to market_news_links.txt\n",
      "links from page 37 have been added to market_news_links.txt\n",
      "links from page 36 have been added to market_news_links.txt\n",
      "links from page 35 have been added to market_news_links.txt\n",
      "links from page 34 have been added to market_news_links.txt\n",
      "links from page 33 have been added to market_news_links.txt\n",
      "links from page 32 have been added to market_news_links.txt\n",
      "links from page 31 have been added to market_news_links.txt\n",
      "links from page 30 have been added to market_news_links.txt\n",
      "links from page 29 have been added to market_news_links.txt\n",
      "links from page 28 have been added to market_news_links.txt\n",
      "links from page 27 have been added to market_news_links.txt\n",
      "links from page 26 have been added to market_news_links.txt\n",
      "links from page 25 have been added to market_news_links.txt\n",
      "links from page 24 have been added to market_news_links.txt\n",
      "links from page 23 have been added to market_news_links.txt\n",
      "links from page 22 have been added to market_news_links.txt\n",
      "links from page 21 have been added to market_news_links.txt\n",
      "links from page 20 have been added to market_news_links.txt\n",
      "links from page 19 have been added to market_news_links.txt\n",
      "links from page 18 have been added to market_news_links.txt\n",
      "links from page 17 have been added to market_news_links.txt\n",
      "links from page 16 have been added to market_news_links.txt\n",
      "links from page 15 have been added to market_news_links.txt\n",
      "links from page 14 have been added to market_news_links.txt\n",
      "links from page 13 have been added to market_news_links.txt\n",
      "links from page 12 have been added to market_news_links.txt\n",
      "links from page 11 have been added to market_news_links.txt\n",
      "links from page 10 have been added to market_news_links.txt\n",
      "links from page 9 have been added to market_news_links.txt\n",
      "links from page 8 have been added to market_news_links.txt\n",
      "links from page 7 have been added to market_news_links.txt\n",
      "links from page 6 have been added to market_news_links.txt\n",
      "links from page 5 have been added to market_news_links.txt\n",
      "links from page 4 have been added to market_news_links.txt\n",
      "links from page 3 have been added to market_news_links.txt\n",
      "links from page 2 have been added to market_news_links.txt\n",
      "links from page 1 have been added to market_news_links.txt\n",
      "links from page 0 have been added to market_news_links.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(163, -1, -1):\n",
    "    getlinks(i, \"market_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data ke JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savejson(new_data, json_file):\n",
    "    # Cek apakah file sudah ada\n",
    "    if os.path.exists(json_file):\n",
    "        try:\n",
    "            # Buka dan baca isi file JSON\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)  \n",
    "                if not isinstance(data, list):  # Pastikan formatnya list\n",
    "                    data = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"⚠️ Terjadi kesalahan saat membaca {json_file}: {e}\")\n",
    "            data = []  # Jika file rusak atau error, buat list baru\n",
    "    else:\n",
    "        data = []  # Jika file belum ada, buat list baru\n",
    "\n",
    "    # Tambahkan data baru ke dalam list\n",
    "    data.append(new_data)\n",
    "\n",
    "    try:\n",
    "        # Simpan kembali ke file JSON\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"✅ Data dengan judul '{new_data['title']}' berhasil disimpan ke {json_file}\")\n",
    "    except OSError as e:\n",
    "        print(f\"❌ Gagal menyimpan data ke {json_file}: {e}\")\n",
    "\n",
    "\n",
    "# ambil data \n",
    "def get_data(url, json_file):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Retry mechanism for handling connection issues\n",
    "    for attempt in range(5):  # Retry up to 5 times\n",
    "        try:\n",
    "            # Request ke halaman berita\n",
    "            response = requests.get(url, headers=headers, timeout=10)  # Add timeout to avoid hanging\n",
    "            \n",
    "            # Cek jika request berhasil (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Mengambil isi artikel dari <div id=\"zoomthis\">\n",
    "                content_element = soup.find(\"div\", id=\"zoomthis\")\n",
    "                if not content_element:\n",
    "                    print(f\"⚠️ Konten tidak ditemukan untuk URL: {url}\")\n",
    "                    return\n",
    "\n",
    "                content = content_element.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "                # Split the content into lines\n",
    "                raw = content.split(\"\\n\")\n",
    "                \n",
    "                # Remove empty lines\n",
    "                raw = [line for line in raw if line.strip()]\n",
    "\n",
    "                if len(raw) < 3:\n",
    "                    print(f\"⚠️ Data tidak lengkap di {url}\")\n",
    "                    return\n",
    "\n",
    "                date = raw[0]\n",
    "                title = raw[1]\n",
    "                text = ' '.join(raw[2:]).strip()\n",
    "\n",
    "                new_data = {\n",
    "                    \"date\": date,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"url\": url\n",
    "                }\n",
    "                \n",
    "                savejson(new_data, json_file)\n",
    "                print(f\"✅ Data berhasil disimpan untuk: {title}\")\n",
    "                return  # Stop retrying if successful\n",
    "\n",
    "            else:\n",
    "                print(f\"❌ Request gagal untuk {url} dengan status: {response.status_code}\")\n",
    "                return  # Stop trying if the server rejects the request\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"⚠️ Koneksi gagal untuk {url} (Percobaan {attempt + 1}/5): {e}\")\n",
    "            time.sleep(random.uniform(2, 5))  # Wait before retrying\n",
    "\n",
    "    print(f\"❌ Gagal mengambil data setelah 5 percobaan: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buka daftar link dan scrap artikel lalu masukkan ke JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and read line by line\n",
    "with open(\"market_news_links_master.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # print(line.strip())  # Strip removes any extra newlines or spaces\n",
    "        get_data(line.strip(), \"NEW_articles_market_news.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and read line by line\n",
    "with open(\"stock_news_links.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # print(line.strip())  # Strip removes any extra newlines or spaces\n",
    "        get_data(line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
